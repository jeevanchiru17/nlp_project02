{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM797vEhEHrH"
      },
      "source": [
        "ASSIGNMENT 2 NLP PROJECT ON WORLD CUP TWEET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPgOJaIiEvVN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aByJEuB7FUPM"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('/Users/jeevanhr/NLP ASSIGNMENT-01/nlp_project01/World Cup tweets/T20_Worldcup_tweets.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "uAayCPlIF30H",
        "outputId": "bf1a09fd-35b1-41e2-cd53-f058399de38c"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W1o8ocaF4a-"
      },
      "outputs": [],
      "source": [
        "# Lowercasing the 'self_text' \n",
        "df['text'] = df['text'].apply(lambda x: x.lower() if isinstance(x, str) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "O_ADiWS4HOYJ",
        "outputId": "31447984-266a-4d7f-9890-6676daf13287"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tSb252QlaAw"
      },
      "source": [
        "html tags removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5S7WBP2Hgc-",
        "outputId": "697ccf36-8858-45ed-ac71-c53f86e68445"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to remove HTML tags\n",
        "def remove_html_tags(text):\n",
        "    if isinstance(text, str):\n",
        "        soup = BeautifulSoup(text, \"html.parser\")\n",
        "        return soup.get_text()\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply the function to the 'self_text' column\n",
        "df['text'] = df['text'].apply(remove_html_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "28-oufQ8Hj-Y",
        "outputId": "340f9a49-7d8e-41e7-bc1e-3b0d303476ef"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPxCVMnGH97h"
      },
      "source": [
        "Removing punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHx_ZIJlHmQp"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply the function to the 'self_text' column\n",
        "df['text'] = df['text'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "MHwmFEtTIISY",
        "outputId": "82f312c6-e4bd-4c06-801b-f2684ec94fbe"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7VCFFxDIYz5"
      },
      "source": [
        "Removing emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMC6O7avIYEv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    if isinstance(text, str):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                               u\"\\U00002500-\\U00002BEF\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "df['text'] = df['text'].apply(remove_emojis)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "b0Rj4boJIjSL",
        "outputId": "c869e340-183c-4998-fb39-d464dc290afa"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfU33dPVJAtN"
      },
      "source": [
        "spelling checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-s8y5Ygnu8o",
        "outputId": "373620d9-844d-4de2-cfc8-998d69d5ee06"
      },
      "outputs": [],
      "source": [
        "pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtUxNa0zJRFk"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Function to perform spell checking\n",
        "def correct_spelling(text):\n",
        "    if isinstance(text, str):\n",
        "        spell = SpellChecker()\n",
        "        words = text.split()\n",
        "        corrected_words = [spell.correction(word) if word.isalpha() else word for word in words]\n",
        "        cleaned_words = [word for word in corrected_words if word is not None and isinstance(word, str)]\n",
        "        return ' '.join(cleaned_words)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Take a smaller sample of the data (e.g., the first 1000 rows)\n",
        "sample_size = 1000\n",
        "sample = df['text'].iloc[:sample_size]\n",
        "\n",
        "# Apply the function to the sample\n",
        "df['text'].iloc[:sample_size] = sample.apply(correct_spelling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "VQmP8dr1JgMP",
        "outputId": "3a2582b9-ab7f-437b-8d44-a479aaa0be61"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN1C-5tPpNlJ"
      },
      "source": [
        "Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCAJlJnTpAEt",
        "outputId": "b23eaf46-6a70-4787-e5cf-ba22991d5745"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLgS2guIOY69",
        "outputId": "f3cf217e-bb07-48b4-b70a-237f3a56a417"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources (if you haven't already)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the 'self_text' column\n",
        "df['text_tokens'] = df['text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zGNaNWI6Od2e",
        "outputId": "839447c3-ed82-4e47-832a-6ba267d2b0a4"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwDjbzoeO1tO"
      },
      "source": [
        "Stop word removal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEyaF4OiOm6I",
        "outputId": "c4f2b1cf-f31b-4397-b1e1-951b017112ff"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords (if you haven't already)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "        return filtered_tokens\n",
        "    else:\n",
        "        return tokens\n",
        "\n",
        "# Apply stopword removal to the tokenized column\n",
        "df['text_tokens_without_stopwords'] = df['text_tokens'].apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Vs0aqei_O7LI",
        "outputId": "76a3d50c-870e-46ec-9d66-146b6f7a7ff0"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcabW0YJpnmD"
      },
      "source": [
        "Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hLnbvBKO8Uk",
        "outputId": "0a4060df-e739-4d55-d071-d4ea7b5c0484"
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources (if you haven't already)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLYbD4MPPO_0"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Function to perform stemming\n",
        "def perform_stemming(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        stemmed_tokens = [porter.stem(word) for word in tokens]\n",
        "        return stemmed_tokens\n",
        "    else:\n",
        "        return tokens\n",
        "\n",
        "# Apply stemming to the tokens without stopwords\n",
        "df['text_stemmed'] = df['text_tokens_without_stopwords'].apply(perform_stemming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "g0PmFml8PTAx",
        "outputId": "45a96878-699e-45f4-ed9e-a1535553041b"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgifvBmfpyAf"
      },
      "source": [
        "Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRIB3_IAPgd2"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to perform lemmatization\n",
        "def perform_lemmatization(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "        return lemmatized_tokens\n",
        "    else:\n",
        "        return tokens\n",
        "\n",
        "# Apply lemmatization to the stemmed tokens\n",
        "df['text_lemmatized'] = df['text_stemmed'].apply(perform_lemmatization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "KLOChxk_Ps9u",
        "outputId": "dba21b51-a435-446e-f968-b5e0b64589ee"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XsEFsQrr8fw"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8T6wYY-Pz8Q"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Function to find sentiment polarity and label\n",
        "def find_sentiment(text):\n",
        "    if isinstance(text, list):\n",
        "        analysis = TextBlob(' '.join(text))\n",
        "        polarity = analysis.sentiment.polarity\n",
        "\n",
        "        # Classify sentiment based on polarity\n",
        "        if polarity > 0:\n",
        "            sentiment = 'Positive'\n",
        "        elif polarity < 0:\n",
        "            sentiment = 'Negative'\n",
        "        else:\n",
        "            sentiment = 'Neutral'\n",
        "\n",
        "        return polarity, sentiment\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Apply sentiment analysis and labeling\n",
        "df['sentiment_polarity'], df['sentiment'] = zip(*df['text_lemmatized'].apply(find_sentiment))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "Nn0dKHZ5Qc5-",
        "outputId": "8ce9c466-a2d5-4daa-9239-2f948bc1a938"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8HgPzzfQ1EG"
      },
      "outputs": [],
      "source": [
        "#df.to_csv('world_cup001.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egWcDJIvsyrt"
      },
      "source": [
        "LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwFODVtvsDs",
        "outputId": "c3c77146-44d5-425d-baa0-a2ceb1075b71"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Assuming 'text_lemmatized' contains preprocessed and tokenized text\n",
        "text_data = df['text_lemmatized'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
        "\n",
        "# Vectorizing text data\n",
        "vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
        "tf = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Applying LDA\n",
        "num_topics = 5  # Number of topics to identify (adjust as needed)\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda.fit(tf)\n",
        "\n",
        "# Transforming text data into topic distributions\n",
        "topic_distributions = lda.transform(tf)\n",
        "\n",
        "# Adding topic distribution features to the DataFrame\n",
        "for i in range(num_topics):\n",
        "    df[f\"topic_{i+1}_distribution\"] = topic_distributions[:, i]\n",
        "\n",
        "# Display the DataFrame with topic distribution features\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82zIh5CHt1ER"
      },
      "source": [
        "Contextual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extracting topic distribution columns\n",
        "topic_columns = [col for col in df.columns if col.startswith('topic_')]\n",
        "\n",
        "# Creating a DataFrame for topic distributions\n",
        "topic_df = df[topic_columns]\n",
        "\n",
        "# Plotting topic distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "topic_df.sum().plot(kind='bar', stacked=True)\n",
        "plt.title('Topic Distribution')\n",
        "plt.xlabel('Topics')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Function to extract hashtags and words from user_description\n",
        "def extract_hashtags_and_words(text):\n",
        "    if isinstance(text, str):\n",
        "        words = text.split()\n",
        "        hashtags = [word.strip(\"#\") for word in words if word.startswith(\"#\")]\n",
        "        return hashtags, words\n",
        "    else:\n",
        "        return [], []\n",
        "\n",
        "# Apply the function to extract hashtags and words\n",
        "df['hashtags'], df['description_words'] = zip(*df['user_description'].apply(extract_hashtags_and_words))\n",
        "\n",
        "# Count word frequencies next to hashtags\n",
        "word_freq_next_to_hashtags = Counter()\n",
        "for hashtags, words in zip(df['hashtags'], df['description_words']):\n",
        "    for i, tag in enumerate(hashtags):\n",
        "        if i < len(words) - 1:\n",
        "            word_freq_next_to_hashtags.update([f\"{tag}_{words[i+1]}\"])\n",
        "\n",
        "# Get top 10 repeated words next to hashtags\n",
        "top_10_words_next_to_hashtags = word_freq_next_to_hashtags.most_common(10)\n",
        "print(top_10_words_next_to_hashtags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('world_cup001.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqVoVIOmt8d5"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
